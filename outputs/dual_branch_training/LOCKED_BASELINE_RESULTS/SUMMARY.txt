
SUMMARY

Date: 2026-01-28T13:33:20.435287

APPROACH:
---------
Baseline dual-branch CNN with weighted CrossEntropyLoss

Architecture:
  - Branch A: Temporal CNN (medium depth, 4 conv layers)
    * Conv1D layers: 32 → 64 → 128 → 256 channels
    * Global average pooling
    * 64-dimensional embedding
  
  - Branch B: Feature encoder (2 dense layers)
    * Input: 30 engineered EEG features
      - Band powers (delta, theta, alpha, beta, gamma)
      - Relative band powers
      - Band ratios (theta/alpha, delta/beta, theta/beta)
      - Spectral entropy, Hjorth parameters
      - Statistical features
    * 32-dimensional embedding
  
  - Fusion: Concatenation (96-dim) → Classifier
  - Total parameters: 470,563

Loss Function:
  - Weighted CrossEntropyLoss
  - Balanced class weights
  - No aggressive scaling

Training:
  - Epochs: 25
  - Time: 80.7 minutes
  - Early stopping on validation macro F1

RESULTS:
--------
Test Performance:
  - Accuracy:      0.3063
  - F1 (macro):    0.2849
  - F1 (weighted): 0.3137

Per-Class Performance:
  - Class 0: Recall=0.2367, F1=0.3373
  - Class 1: Recall=0.5145, F1=0.3377
  - Class 2: Recall=0.2363, F1=0.1798

Prediction Distribution:
  - Class 0: 3492 predictions
  - Class 1: 7415 predictions
  - Class 2: 3543 predictions

All classes predicted: True

INTERPRETATION:
--------------
Baseline dual-branch CNN with weighted CrossEntropy shows limited class separability.

The model demonstrates successful multi-class 
learning with macro F1 of 0.2849. 

All three classes receive predictions, indicating the model has learned discriminative patterns for each class.

Key observations:
  - Temporal CNN (Branch A) captures local signal patterns
  - Feature encoder (Branch B) preserves domain knowledge
  - Fusion layer combines both representations
  - Performance is balanced across classes

LIMITATIONS:
-----------
  - CNN-only temporal learning may miss long-range dependencies
  - No recurrent mechanism to capture temporal dynamics
  - EEG signals have inherent temporal structure not fully exploited
  
NEXT STEPS:
----------
  - Add BiLSTM to Branch A for temporal dependency modeling
  - This represents a shift to representation learning
  - Expected improvement in minority class performance


CONFUSION MATRIX:
----------------
[[2048 4451 2153]
 [ 883 1864  876]
 [ 561 1100  514]]

Normalized (by true class):
[[0.23670828 0.51444753 0.2488442 ]
 [0.24372067 0.51449075 0.24178857]
 [0.25793103 0.50574713 0.23632184]]
